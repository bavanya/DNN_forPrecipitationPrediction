{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install torch torchvision\n",
    "#pip3 install scipy\n",
    "#pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from sklearn_xarray import wrap\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the data for the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preparation(scaler):\n",
    "    \n",
    "    \"\"\"Deserializing the pickle file\"\"\"\n",
    "    df = pd.read_pickle(\"./wikilimoData.pkl\")\n",
    "    \n",
    "    \"\"\"Dropping the rows which have null values\n",
    "    Instead of interpolating we are dropping null values.\n",
    "    Since we have sufficiently large data, we can affort to lose \n",
    "    some of the rows, instead of compromising the accuracy\n",
    "    of the dataset by interpolating them. \n",
    "    \"\"\"\n",
    "    df= df.dropna()\n",
    "    \n",
    "    \"\"\"Appying log tranform to remove skewness of total precipitation\n",
    "    column values\n",
    "    \n",
    "    Must remember to apply inverse log to the prediction obtained to get \n",
    "    actual values\n",
    "    \"\"\"\n",
    "    df['tp'] = np.log(df['tp'])\n",
    "    \n",
    "    \"\"\"Performing min max normalization to \n",
    "    avoid higher influence of temperature at 2m compared to\n",
    "    volumetric soil water layer on predicting the precipitation\n",
    "    of a region.\"\"\"\n",
    "    df[list(df.columns)] = scaler.fit_transform(df[list(df.columns)])\n",
    "    \n",
    "    \"\"\"Taking respective columns for input and output of the model.\"\"\"\n",
    "    X = df[['t2m', 'swvl1']]\n",
    "    y = df[['tp']]\n",
    "    \n",
    "    return df, X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splittingData(X, y, random_state, test_size):\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=random_state, test_size=test_size)\n",
    "    \n",
    "    x_train = Variable(torch.from_numpy(np.array(x_train.astype(float)))).type(torch.FloatTensor)\n",
    "    y_train = Variable(torch.from_numpy(np.array(y_train.astype(float)))).type(torch.FloatTensor)\n",
    "    x_test = Variable(torch.from_numpy(np.array(x_test.astype(float)))).type(torch.FloatTensor)\n",
    "    y_test = Variable(torch.from_numpy(np.array(y_test.astype(float)))).type(torch.FloatTensor)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "df, X, y = data_preparation(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = splittingData(X, y, 42, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [0.7333],\n",
       "        [0.0000],\n",
       "        ...,\n",
       "        [0.5198],\n",
       "        [0.3551],\n",
       "        [0.0000]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building the model**\n",
    "\n",
    "From the data visualization through plotting, we understood that \n",
    "the total precipitation of a specific geo spatial point varies wrt \n",
    "to the date highly but doesn't change much when comparing\n",
    "wrt to latitude and logitude, except for a few regions.\n",
    "\n",
    "But since the temperature at 2m and volumetric soil water layer also\n",
    "vary wrt to the time and latitude and longitude variables,\n",
    "we will build considering the simple case of just ignoring the \n",
    "time and geo spatial variables and predicting the total precipitation by considering\n",
    "dependence only on temperature at 2m and volumetric soil water layer.\n",
    "\n",
    "Since we have sufficiently large data, we can afford to take\n",
    "large % of dataset for training, but considering \n",
    "the computation power available to build the model\n",
    "we are taking the split to be 80:20 for training  and\n",
    "testing.\n",
    "\n",
    "Splitting the data.\n",
    "80% for training\n",
    "20% for testing and evaluating the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying sigmoid is done as we have normalized total precipitation in the preprocessing step, sigmoid ensures that the output of the model is also between 0 and 1 like the normalized \n",
    "total precipitation actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        \n",
    "        \"\"\"parameters\"\"\"\n",
    "        self.inputSize = 2\n",
    "        self.outputSize = 1\n",
    "        self.hiddenSize = 3\n",
    "        \n",
    "        \"\"\"weights\n",
    "        Initialized with random values\n",
    "        \"\"\"\n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize) # 3 X 2 tensor\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize) # 3 X 1 tensor\n",
    "        \n",
    "    def forward(self, X):\n",
    "        self.z = torch.matmul(X, self.W1) \n",
    "        self.z2 = self.sigmoid(self.z) # activation function\n",
    "        self.z3 = torch.matmul(self.z2, self.W2)\n",
    "        \"\"\"Applying sigmoid before taking output because since we normalized y_train we know\n",
    "        that the values are between 0 and 1, hence applying sigmoid guarantees this.\"\"\"\n",
    "        o = self.sigmoid(self.z3) # final activation function\n",
    "        return o\n",
    "        \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        \"\"\"derivative of sigmoid\"\"\"\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def backward(self, X, y, o):\n",
    "        self.o_error = y - o\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o)\n",
    "        \n",
    "        self.z2_error = torch.matmul(self.o_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.o_delta)\n",
    "        \n",
    "    def train(self, X, y):\n",
    "        \"\"\"forward + backward pass for training\"\"\"\n",
    "        o = self.forward(X)\n",
    "        self.backward(X, y, o)\n",
    "        \n",
    "    def saveWeights(self, model):\n",
    "        torch.save(model, \"NN\")\n",
    "        \n",
    "    def predict(self, x_test):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(x_test))\n",
    "        y_pred = self.forward(x_test)\n",
    "        print (\"Output: \\n\" + str(y_pred))\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 0.4006926119327545\n",
      "#1 Loss: 0.10465307533740997\n",
      "#2 Loss: 0.10465307533740997\n",
      "#3 Loss: 0.10465307533740997\n",
      "#4 Loss: 0.10465307533740997\n",
      "#5 Loss: 0.10465307533740997\n",
      "#6 Loss: 0.10465307533740997\n",
      "#7 Loss: 0.10465307533740997\n",
      "#8 Loss: 0.10465307533740997\n",
      "#9 Loss: 0.10465307533740997\n"
     ]
    }
   ],
   "source": [
    "NN = Neural_Network()\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    \"\"\"Displaying mean squared loss\"\"\"\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y_train - NN(x_train))**2).detach().item()))\n",
    "    NN.train(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining predictions for x_test using the first model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted data based on trained weights: \n",
      "Input (scaled): \n",
      "tensor([[0.3150, 0.2658],\n",
      "        [0.8097, 0.4928],\n",
      "        [0.3558, 0.2662],\n",
      "        ...,\n",
      "        [0.3839, 0.3997],\n",
      "        [0.1419, 0.4237],\n",
      "        [0.2067, 0.4194]])\n",
      "Output: \n",
      "tensor([[0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        ...,\n",
      "        [0.5000],\n",
      "        [0.5000],\n",
      "        [0.5000]])\n"
     ]
    }
   ],
   "source": [
    "#NN.saveWeights(NN)\n",
    "y_pred_tensor = NN.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = plt.axes(aspect='equal')\n",
    "plt.scatter(y_test.numpy(), y_pred_tensor.numpy())\n",
    "plt.xlabel('True Values [MPG]')\n",
    "plt.ylabel('Predictions [MPG]')\n",
    "lims = [0, 1]\n",
    "plt.xlim(lims)\n",
    "plt.ylim(lims)\n",
    "_ = plt.plot(lims, lims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the scatter plot, we see that all the values are clustered at\n",
    "0.5 , this is a very serious case of underfitting.\n",
    "\n",
    "Since it is underfitting, we have to add a few more layers and re built the model.\n",
    "\n",
    "Since we developed a very simple model, there is no need of using \n",
    "aggressive techniques to avoid overfitting like optmizers, using momentum variable to gradually reduce the learning rate.\n",
    "\n",
    "We could also add weight regularization to the loss function, to avoid overfitting but since our model is underfitting and not \n",
    "overfitting, we have not added it.\n",
    "\n",
    "Also could have considered adding drop out to few of the layers.\n",
    "\n",
    "Also we could have added few combinations of the above techniques together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation of the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_numpy = y_pred_tensor.numpy()\n",
    "y_pred_dataframe = pd.DataFrame(y_pred_numpy)\n",
    "    \n",
    "y_test_numpy = y_test.numpy()\n",
    "y_test_dataframe = pd.DataFrame(y_test_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = mean_squared_error(y_test_numpy, y_pred_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean squared error: 0.104631\n"
     ]
    }
   ],
   "source": [
    "print('\\nMean squared error: %f'%MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking MSE as the metric, evaluate the model.\n",
    "Inputs:\n",
    "1.y_test_dataframe 2. y_pred_dataframe\n",
    "\n",
    "Not taking Mean absolute precentage error as metric here since few of values are 0,\n",
    "hence MAPE would give divide by zero error and would not be able to evaluate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "error = y_pred_numpy - y_test_numpy\n",
    "plt.hist(error, bins=25)\n",
    "plt.xlabel('Prediction Error [MPG]')\n",
    "_ = plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram to visualize the error distribution\n",
    "\n",
    "From the histogram we see that most of the prediction error is less than zero and for huge cases it is between 0.4 and 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inversing the transformations and scaling to actual values of total precipitation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transformation_and_scaling(y_pred_dataframe, y_test_dataframe):\n",
    "    \n",
    "    y_pred_dataframe['My new column'] = 0\n",
    "    y_pred_dataframe['My new column1'] = 0\n",
    "    y_pred_dataframe = scaler.inverse_transform(y_pred_dataframe)[:, [0]]\n",
    "    y_actual_predictions = np.exp(y_pred_dataframe)\n",
    "    \n",
    "    y_test_dataframe['My new column'] = 0\n",
    "    y_test_dataframe['My new column1'] = 0\n",
    "    y_test_dataframe = scaler.inverse_transform(y_test_dataframe)[:, [0]]\n",
    "    y_test_original = np.exp(y_test_dataframe)\n",
    "    \n",
    "    return y_test_original, y_actual_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_original, y_actual_predictions = inverse_transformation_and_scaling(y_pred_dataframe, y_test_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building 2nd model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DROPOUT_PROB = 0.9\n",
    "\n",
    "LR = 0.001\n",
    "MOMENTUM= 0.99\n",
    "dropout = torch.nn.Dropout(p=1 - (DROPOUT_PROB))\n",
    "\n",
    "hiddenLayer1Size=64\n",
    "hiddenLayer2Size=int(hiddenLayer1Size/4)\n",
    "\n",
    "#Neural Network layers\n",
    "linear1=torch.nn.Linear(2, hiddenLayer1Size, bias=True) \n",
    "linear2=torch.nn.Linear(hiddenLayer1Size, hiddenLayer2Size)\n",
    "linear3=torch.nn.Linear(hiddenLayer2Size, 1)\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "threshold = nn.Threshold(0.5, 0)\n",
    "relu=torch.nn.LeakyReLU()\n",
    "\n",
    "#Neural network architecture\n",
    "net = torch.nn.Sequential(linear1,relu,\n",
    "                          linear2,dropout,relu,\n",
    "                          linear3,relu,\n",
    "                          sigmoid\n",
    "                          )\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-3)\n",
    "loss_func=torch.nn.MSELoss()\n",
    "epochs = 1\n",
    "all_losses = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding drop out, momentum, weight decay for better performance of the model.\n",
    "\n",
    "Adding relu activation layer after every layer except the last layer as \n",
    "we dont want to restrict the values obtained between the layers between 0 and 1 \n",
    "but we want them all to be postive.\n",
    "\n",
    "And since we want the output to be between 0 and 1 only we are applying sigmoid at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the 2nd model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training in batches\n",
    "for step in range(epochs):    \n",
    "    out = net(x_train).data                 # input x and predict based on x\n",
    "    cost = loss_func(out, y_train) \n",
    "    optimizer.zero_grad()   # clear gradients for next train\n",
    "    cost.backward()         # backpropagation, compute gradients\n",
    "    optimizer.step()        # apply gradients \n",
    "               \n",
    "    loss = cost\n",
    "    all_losses.append(loss)\n",
    "    print(step, cost.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Obtaining predictions using the 2nd model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = net(x_test).data      \n",
    "print (\"Pred:\" + str (prediction))      \n",
    "pred_y = prediction.numpy().squeeze()\n",
    "target_y = y_test.numpy()\n",
    "print ('Mean squared error ={} '.format(mean_squared_error(target_y, pred_y))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could not train the 2nd model because of limited computation power.\n",
    "\n",
    "If it were overfitting we would have removed a hidden layer or changed the layer size and checked.\n",
    "\n",
    "We could have taken the following arcitecture:\n",
    "\n",
    "input layer (None, 3)                      \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 32)                     \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 16)                     \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                      \n",
    "_________________________________________________________________\n",
    "\n",
    "Or: \n",
    "\n",
    "input layer (Normalization (None, 3)                  \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 64)            \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                      \n",
    "_________________________________________________________________\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
